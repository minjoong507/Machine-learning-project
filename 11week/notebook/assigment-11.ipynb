{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xe9MCldoHIl9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOWZlne2HiS8"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiB4-X2OHuOZ"
   },
   "outputs": [],
   "source": [
    "data_path = './MNIST'\n",
    "\n",
    "data_test   = datasets.MNIST(root = data_path, train= True, download=True, transform= transform)\n",
    "data_train  = datasets.MNIST(root = data_path, train= False, download=True, transform= transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3aWGS6JHzU7"
   },
   "outputs": [],
   "source": [
    "print(\"the number of your training data (must be 10,000) = \", data_train.__len__())\n",
    "print(\"hte number of your testing data (must be 60,000) = \", data_test.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSJDSPfCIONM"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size = 64, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=64, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktk_eY-9NOf1"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def calc_out(in_layers, stride, padding, kernel_size, pool_stride):\n",
    "    \"\"\"\n",
    "    Helper function for computing the number of outputs from a\n",
    "    conv layer\n",
    "    \"\"\"\n",
    "    return int((1+(in_layers - kernel_size + (2*padding))/stride)/pool_stride)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Some helpful values\n",
    "        inputs      = [1,32,64,64] # MNIST data shape\n",
    "        kernel_size = [5,5,3]\n",
    "        stride      = [1,1,1]\n",
    "        pool_stride = [2,2,2]\n",
    "\n",
    "        # Layer lists\n",
    "        layers = []\n",
    "\n",
    "        self.out   = 28\n",
    "        self.depth = inputs[-1]\n",
    "        for i in range(len(kernel_size)):\n",
    "            # Get some variables\n",
    "            padding = int(kernel_size[i]/2)\n",
    "\n",
    "            # Define the output from this layer\n",
    "            self.out = calc_out(self.out, stride[i], padding,\n",
    "                                kernel_size[i], pool_stride[i])\n",
    "\n",
    "            # convolutional layer 1\n",
    "            layers.append(nn.Conv2d(inputs[i], inputs[i+1], kernel_size[i], \n",
    "                                       stride=stride[i], padding=padding))\n",
    "            layers.append(nn.BatchNorm2d(inputs[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            # convolutional layer 2\n",
    "            layers.append(nn.Conv2d(inputs[i+1], inputs[i+1], kernel_size[i], \n",
    "                                       stride=stride[i], padding=padding))\n",
    "            layers.append(nn.BatchNorm2d(inputs[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            # maxpool layer\n",
    "            layers.append(nn.MaxPool2d(pool_stride[i],pool_stride[i]))\n",
    "            layers.append(nn.Dropout(p=0.2))\n",
    "\n",
    "        self.cnn_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        print(self.depth*self.out*self.out)\n",
    "        \n",
    "        # Now for our fully connected layers\n",
    "        layers2 = []\n",
    "        layers2.append(nn.Dropout(p=0.2))\n",
    "        layers2.append(nn.Linear(self.depth*self.out*self.out, 512))\n",
    "        layers2.append(nn.Dropout(p=0.2))\n",
    "        layers2.append(nn.Linear(512, 256))\n",
    "        layers2.append(nn.Dropout(p=0.2))\n",
    "        layers2.append(nn.Linear(256, 256))\n",
    "        layers2.append(nn.Dropout(p=0.2))\n",
    "        layers2.append(nn.Linear(256, 10))\n",
    "\n",
    "        self.fc_layers = nn.Sequential(*layers2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(-1, self.depth*self.out*self.out)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2f5O7Lba4JY"
   },
   "outputs": [],
   "source": [
    "tLoss, vLoss = [], []\n",
    "tacc, vacc = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4A6JGhnToSJ"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcIXcv5TT9O-"
   },
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "# Get the device\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    train_correct = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    # train #\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data   = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "        prediction = output.data.max(1)[1]   # first column has actual prob.\n",
    "        correct = prediction.eq(target.data).sum()\n",
    "        train_correct += correct\n",
    "    \n",
    " \n",
    "    # test #\n",
    "    model.eval()\n",
    "    for data, target in test_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data   = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()*data.size(0)\n",
    "\n",
    "        prediction = output.data.max(1)[1]   # first column has actual prob.\n",
    "        correct = prediction.eq(target.data).sum()\n",
    "        test_correct += correct\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    tLoss.append(train_loss)\n",
    "    vLoss.append(test_loss)\n",
    "\n",
    "    # calculate average accuracy\n",
    "    train_accuracy = train_correct / len(train_loader.dataset)\n",
    "    test_accuracy = test_correct / len(test_loader.dataset)\n",
    "    tacc.append(train_accuracy)\n",
    "    vacc.append(test_accuracy)\n",
    "\n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.5f} \\tTesting Loss: {:.5f}'.format(\n",
    "        epoch, train_loss, test_loss))\n",
    "    print('Epoch: {} \\tTraining acc: {:.5f} \\tTesting acc: {:.5f}'.format(\n",
    "        epoch, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpmdmUORd1mH"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSdIL-pkd3Zx"
   },
   "source": [
    "### 1. Plot the training and testing losses over epochs [2pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixd-K2TBd2uE"
   },
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(tLoss))], tLoss, label = 'training loss' , c = 'blue')\n",
    "plt.plot([i for i in range(len(vLoss))], vLoss, label = 'testing loss' , c = 'red')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"training and testing losses over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EV7Gn7ejeWRW"
   },
   "source": [
    "### 2. Plot the training and testing accuracies over epochs [2pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83LWmt3ceFkf"
   },
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(tacc))], tacc, label = 'training acc' , c = 'blue')\n",
    "plt.plot([i for i in range(len(vacc))], vacc, label = 'testing acc' , c = 'red')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"training and testing accuracies over epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGhUXerbehaL"
   },
   "source": [
    "### 3. Print the final training and testing losses at convergence [2pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1shqwTv2ejm8"
   },
   "outputs": [],
   "source": [
    "print('Training loss: {:.5f} \\nTesting loss: {:.5f}'.format(tLoss[-1], vLoss[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeCW3F4sejxK"
   },
   "source": [
    "### 4. Print the final training and testing accuracies at convergence [20pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJGiS_UEed0Y"
   },
   "outputs": [],
   "source": [
    "print('Training acc: {:.5f} \\nTesting acc: {:.5f}'.format(tacc[-1], vacc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg1u4w2Qel9g"
   },
   "source": [
    "### 5. Print the testing accuracies within the last 10 epochs [5pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qb_H5V78fVlo"
   },
   "outputs": [],
   "source": [
    "vacc_last_10_epochs = vacc[n_epochs - 10:]\n",
    "\n",
    "for i in range(len(vacc_last_10_epochs)):\n",
    "    print(\"[epoch = {}] \\tTesting acc : {:.5f}\".format(n_epochs - 10 + i, vacc_last_10_epochs[i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled1.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
